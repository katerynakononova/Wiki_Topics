{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "народитися дивитися категорія народитися грехема макферсон англійський співак джулія дрейфус американський актриса майя чибурданідзе грузинський шахістка уейн грецкий канадський хокеїст лютий геннадій куцак заслужений лютий вінс нейл американський співак лютий енді тейлор англійський музикант лютий коротаєв андрій віталійович історик філософ соціолог орієнталіст лютий джон американський музикант олена олексіївна яковлев російський кіноактриса лотар маттеус німецький футболіст олександр цекати співак василь радянський футболіст едді мерфі актор олександр заваров футболіст травень джей астон співак травень джордж клуня американський актор травень кемпбелл музикант травень денніс родмен американський баскетболіст травень англійський кіноактор травень томпсон актриса майкл фокс актор джордж англійський співак елісон співачка наталія марія петрівна фарина співачка сопрано педагог заслужений артистка\n"
     ]
    }
   ],
   "source": [
    "# Importing prepared dataset\n",
    "import pickle\n",
    "input = open('my_corpus.pql', 'rb')\n",
    "obj = pickle.load(input)\n",
    "input.close()\n",
    "\n",
    "corpus = obj['corpus']\n",
    "words_list = obj['words_list']\n",
    "all_words = obj['all_words']\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "w1, w2 = train_test_split(words_list, test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('ubercorpus.lowercased.lemmatized.word2vec.300d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_embed(words, corpus):\n",
    "    x_sent_embed = []\n",
    "    count_words, count_non_words = 0, 0  \n",
    "    \n",
    "    # recover the embedding of each sentence with the average of the vector that composes it\n",
    "    # sent - sentence\n",
    "    for sent in corpus:\n",
    "        # average embedding of all words in a sentence\n",
    "        sent_embed = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # if word is present in the dictionary - add its vector representation\n",
    "                count_words += 1\n",
    "                sent_embed.append(model[word])\n",
    "            except KeyError:\n",
    "                # if word is not in the dictionary - add a zero vector\n",
    "                count_non_words += 1\n",
    "                sent_embed.append([0] * 300)\n",
    "        \n",
    "        # add a sentence vector to the list\n",
    "        x_sent_embed.append(np.mean(sent_embed, axis=0).tolist())\n",
    "                    \n",
    "    print(count_non_words, \"out of\", count_words, \"words were not found in the vocabulary.\")\n",
    "    \n",
    "    return x_sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44333 out of 367958 words were not found in the vocabulary.\n",
      "47963 out of 372782 words were not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "x1 = txt_embed(words, w1)\n",
    "x2 = txt_embed(words, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed1 = []\n",
    "for i,v in enumerate(x1): \n",
    "    if np.any(np.isnan(v)) != True:\n",
    "        embed1.append(np.array(v))\n",
    "    else:\n",
    "        \n",
    "        embed1.append(np.array([0] * 300))\n",
    "        print(i)\n",
    "em1 = np.vstack(embed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed2 = []\n",
    "for i,v in enumerate(x2): \n",
    "    if np.any(np.isnan(v)) != True:\n",
    "        embed2.append(np.array(v))\n",
    "    else:\n",
    "        embed2.append(np.array([0] * 300))\n",
    "        print(i)\n",
    "em2 = np.vstack(embed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model1, model2):\n",
    "    rand = adjusted_rand_score(model1,model2).round(3)\n",
    "    homo = homogeneity_score(model1,model2).round(3)\n",
    "    vm = v_measure_score(model1,model2).round(3)\n",
    "    print(rand, homo, vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Clusters\n",
    "k = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0 0.008 0.008\n"
     ]
    }
   ],
   "source": [
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "km1 = KMeans(n_clusters = k).fit(em1)\n",
    "km2 = KMeans(n_clusters = k).fit(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "metrics(km1.labels_.tolist(),km2.labels_.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.009 0.009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc1 = AgglomerativeClustering(n_clusters = k, affinity = 'euclidean', linkage = 'ward').fit_predict(em1)\n",
    "hc2 = AgglomerativeClustering(n_clusters = k, affinity = 'euclidean', linkage = 'ward').fit_predict(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "metrics(hc1,hc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.006 0.003 0.001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "mshift1 = MeanShift(bandwidth=11).fit(em1)\n",
    "mshift2 = MeanShift(bandwidth=11).fit(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "metrics(mshift1.labels_,mshift2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mshift1.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007 0.007 0.009\n"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "hdb1 = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=7).fit(em1)\n",
    "hdb2 = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=7).fit(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "metrics(hdb1.labels_,hdb2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(hdb2.labels_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
